from collections import defaultdict
from collections.abc import Sequence
from itertools import pairwise

from openhands.sdk.context.view.manipulation_indices import ManipulationIndices
from openhands.sdk.context.view.properties.base import ViewPropertyBase
from openhands.sdk.event import ActionEvent, Event, EventID, LLMConvertibleEvent


class BatchAtomicityProperty(ViewPropertyBase):
    """Ensures all events from the same batch (sharing the same llm_response_id) form an
    atomic unit.

    When an LLM makes a single response containing multiple tool calls, those calls are
    considered semantically related. However, we split each tool call into a separate
    event, and so to reproduce the original message we must maintain all events from the
    same batch. If we forget any one of those events (via condensation, say), then we
    must forget all of them to maintain consistency.
    """

    def enforce(
        self,
        current_view_events: list[LLMConvertibleEvent],
        all_events: Sequence[Event],
    ) -> set[EventID]:
        """Enforce batch atomicity by removing all events from a partially-removed
        batch.

        If any ActionEvent in a batch is missing, this method will mark all other
        ActionEvent objects from that batch for removal. Relies on all_events to detect
        and identify batches.
        """
        all_batches = self._build_batches(all_events)
        events_to_remove: set[EventID] = set()

        for llm_response_id, view_batch_ids in self._build_batches(
            current_view_events
        ).items():
            # We assume that the current view events are a strict subset of the elements
            # of the all_events sequence -- if the batch ids in the view aren't exactly
            # one-to-one with the batch ids generated by the all_events sequence, that
            # can only mean something has been forgotten and we need to drop the entire
            # batch.
            if view_batch_ids != all_batches[llm_response_id]:
                events_to_remove.update(view_batch_ids)

        return events_to_remove

    def manipulation_indices(
        self,
        current_view_events: list[LLMConvertibleEvent],
    ) -> ManipulationIndices:
        """Calculate manipulation indices that respect batch atomicity.

        Within a batch (from the start index to the end), no manipulation is allowed, so
        the manipulation indices lie on the batch boundaries.
        """
        # We'll start with a complete set of manipulation indices and remove the
        # inter-batch indices.
        manipulation_indices: ManipulationIndices = ManipulationIndices.complete(
            current_view_events
        )

        for index, (left, right) in enumerate(pairwise(current_view_events)):
            # If the left and right event correspond to action events with the same LLM
            # response ID, they're part of the same batch. We need to remove the index
            # between them -- the enumeration index corresponds to the index for `left`,
            # so we remove `index + 1`.
            if (
                isinstance(left, ActionEvent)
                and isinstance(right, ActionEvent)
                and left.llm_response_id == right.llm_response_id
            ):
                manipulation_indices.remove(index + 1)

        return manipulation_indices

    def _build_batches(self, events: Sequence[Event]) -> dict[EventID, set[EventID]]:
        """Utility function that builds a map from LLM response IDs to the event IDs of
        actions in that batch.
        """
        batches: dict[EventID, set[EventID]] = defaultdict(set)

        for event in events:
            if isinstance(event, ActionEvent):
                batches[event.llm_response_id].add(event.id)

        return batches
